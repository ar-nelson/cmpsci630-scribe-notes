People have been talking about the end opf disks for a long time, but we still have disks. Some laptops have flash, or SSD (solid-state), drives, which have lots of different characteristics from traditional "spinning media."

How SSDs are different:
- O(1) random access

SSds are also known as NVRAM (non-volatile RAM), meaning RAM that doesn'tr go away when it loses power.

How SSDs are: you have chunks, you apply voltage to them, their values "stick." RAM has a clock rate, new information is sent to it every clock tick, if new information isn't sent then the memory is lost.

Other NVRAM technology:
PCM - phase-change memory. Not too different from DVD/CD technology. Heat causes a crystal to change state.
memristers - ?

DVD/CD: Old CD burners would literally etch holes in the CD. A laser checks where the etched pits are. As opposed to traditional spinning media, which is magnetic: a head floats above the surface of the disk, magnetizing the head writes bits, detecting head movement reads bits. Magnetic drives are usually vacuum-sealed, although floppy drives weren't...

As density increases, risk of error goes up. As soon as the vacuum seal of a hard drive is open, it's dead. Even smoke particles are big enough to cause issues. Optical media is more reliable, but also slow. A DVD drive, to be as fast as an average hard drive, woud have to have a speed of ~1000X, which would destroy the disc. Hard drives can acceerate to ridiculous speeds; 10,000 RPM is normal in server drives. This is why we still use spinning media: high bandwidth. But... compared to NVRAM, they also have high latency.

Google switched almost entirely from spinning disks to SSDs. Why?

- power savings
    - hard drive ~10W
    - SSD ~mw
- predictable wear
- data access speed

A hard drive is a stack of spinning discs, each of which has concentric rings of bits. The magnetic head must physically move when switching between rings, which slows down random access (seek latency). Random access is also slowed by waiting for the disk to spin around (rotation latency). To reduce this latency, files should be contiguous. Defragmentation (on old Windows machines) was used to keep files as contiguous as possible. Defragmentation might not be used as much today because disks don't usually fill up (how do you use up a terabyte?).

Disks, even when vacuum-sealed, fail. Often. In fact, parts of the magnetic disk on a hard drive often don't work right out of the factory. During the "burn-in" period of a hard drive at the factory (testing it to know whether it works), some NVRAM in the HD is set to identify bad blocks, which tells the hardware to skip them. This may result in "contiguous" blocks not actually being contiguous, because one of the blocks is bad and the drive redirects it to another block in another part of the disk.

The burn-in period is used because most hardware has failure rates that follow a "bathtub graph": most failures occur toward the very beginning or very end of a product's expected lifetime. By checking for the early failures, customers don't see them, and experience a linear failure rate.

Modern SCSI drives have a hardware cache, which is good (performance), but also bad (if the drive loses power before data in cache has been written to disk, the data can be lost!) This can result in "lost blocks," which can lead to catastrophic inconsistency in the disk.

There's an obvious tension between performance and reliability. File systems used to focus exclusively on performance, but reliability has recently been more important. Modern file systems have reliability features such as journaling.

Journaling: Pretend that, at any moment, the power's going to go out, or the disk's going to fail. How do we recover from that? Answer: keep a log of every disk action, then, when recovering from a power failure, if the most recent entry in the log is not the end of a complete action, we can "rewind" to the beginning of this partial action.

Replication: If the chance of failure is p, the chance of success is 1-p. If there are two disks, and only one must succeed for a write to succeed, then the chance of success is 1-p^2... and, if p is small, then p^2 is *really* small. Of course, this is assuming that the chances of failure are independent!

RAID used to mean "Redundant Array of Inexpensive Disks," but now it means "Redundant Array of Independent Disks." Redundancy is useless without independence.

Flash reliability is strange: reads are easy and painless, but writes require huge chunks to be deleted and rewritten at once. But this works well for big data: the more data there is, the more of the load is reads. Flash memory can handle only a limited number of writes before breaking down, but this has become less of a problem because the filesystem cycles writes among all of the blocks ("load leveling"). Contiguousness is not an issue; latency is constant, and there is no seek latency.

Of course, everyone doesn't just move to SSDs, because they're expensive... about 4x the $/GB of traditional drives. But SSDs are used in mobile devices because a) they can take more of a beating, b) they don't use much power, and c) they don't give off much heat.

RAID has two purposes: performance and reliability.
- striping (RAID 0) best cost/performance, worst reliability
- mirroring (RAID 1) best reliability, worst cost/performance
- higher levels: simultaneous mirroring and striping

Striping: split a file up into blocks among multiple disks. Greatly increases performance through parallelism, but also decreases reliability (for striping across two disks, faiure rate is sqrt(p)).

RAID recovery is actually shockingly unreliable. It's basically a false sense of security. This is because, with so many disks, the chance of failure increases significantly. Google uses mirroring, but not higher-level RAID. Google actually still uses tape backup: tape has terrible seek speed, but pretty good bandwidth.

The moral of this class is: MAKE BACKUPS.